{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to your dataset directories\n",
    "image_dir = 'dataset/augmented_data/images'\n",
    "label_dir = 'dataset/augmented_data/labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to save preprocessed data\n",
    "preprocessed_dir = 'preprocessed_data'\n",
    "train_dir = os.path.join(preprocessed_dir, 'train')\n",
    "val_dir = os.path.join(preprocessed_dir, 'val')\n",
    "test_dir = os.path.join(preprocessed_dir, 'test')\n",
    "\n",
    "# Create directories to save preprocessed data\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for image size and batch size\n",
    "IMAGE_SIZE = (256, 256)  # Resize images to this size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data preprocessing function\n",
    "def preprocess_image(image):\n",
    "    # Normalize pixel values to range [0, 1] by dividing by 255\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # Resize images to predefined size\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset using TensorFlow data pipeline\n",
    "def load_and_preprocess_image(image_path, label_path):\n",
    "    # Read image from file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Decode JPEG-encoded image to tensor\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # Preprocess image\n",
    "    image = preprocess_image(image)\n",
    "    return image, label_path, tf.strings.regex_replace(image_path, '.*/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset using TensorFlow data pipeline\n",
    "def load_dataset(image_directory, label_directory):\n",
    "    # Get list of image file paths in the directory\n",
    "    image_paths = tf.data.Dataset.list_files(os.path.join(image_directory, '*.jpg'))\n",
    "    label_paths = tf.data.Dataset.list_files(os.path.join(label_directory, '*.json'))\n",
    "    # Load images and labels\n",
    "    dataset = tf.data.Dataset.zip((image_paths, label_paths))\n",
    "    # Load and preprocess images in parallel using map\n",
    "    dataset = dataset.map(load_and_preprocess_image)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training, validation, and testing sets\n",
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    # Get dataset size\n",
    "    dataset_size = len(dataset)\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(buffer_size=dataset_size)\n",
    "    # Calculate split sizes\n",
    "    train_size = int(train_ratio * dataset_size)\n",
    "    val_size = int(val_ratio * dataset_size)\n",
    "    test_size = int(test_ratio * dataset_size)\n",
    "    # Split dataset\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size).take(val_size)\n",
    "    test_dataset = dataset.skip(train_size + val_size).take(test_size)\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(image_dir, label_dir)\n",
    "\n",
    "# Split dataset into training, validation, and testing sets\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data for training, validation, and testing\n",
    "def save_preprocessed_data(dataset, directory):\n",
    "    images_save_dir = os.path.join(directory, 'images')\n",
    "    labels_save_dir = os.path.join(directory, 'labels')\n",
    "    os.makedirs(images_save_dir, exist_ok=True)\n",
    "    os.makedirs(labels_save_dir, exist_ok=True)\n",
    "    for (image, label_path, image_path) in dataset:\n",
    "        # Extract original file name of the image\n",
    "        image_name = os.path.basename(image_path.numpy()).decode(\"utf-8\")\n",
    "        # Save preprocessed images with original file names\n",
    "        tf.keras.preprocessing.image.save_img(os.path.join(images_save_dir, image_name), image)\n",
    "        # Extract original file name of the label\n",
    "        label_name = os.path.basename(label_path.numpy()).decode(\"utf-8\")\n",
    "        # Copy labels to preprocessed labels directory with original file names\n",
    "        label_dest_path = os.path.join(labels_save_dir, label_name)\n",
    "        shutil.copy(label_path.numpy(), label_dest_path)\n",
    "\n",
    "# Save preprocessed data for training, validation, and testing\n",
    "save_preprocessed_data(train_dataset, train_dir)\n",
    "save_preprocessed_data(val_dataset, val_dir)\n",
    "save_preprocessed_data(test_dataset, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kenv",
   "language": "python",
   "name": "kenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
